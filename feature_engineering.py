"""
Ù…Ø§Ú˜ÙˆÙ„ Feature Engineering
Phase 4: Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯ÛŒØªØ§ Ø¨Ø±Ø§ÛŒ Machine Learning

Ø§ÛŒÙ† Ù…Ø§Ú˜ÙˆÙ„:
1. Feature Ù‡Ø§ÛŒ Ù„Ø§Ø²Ù… Ø±Ùˆ Ø§Ø² Ø¯ÛŒØªØ§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒâ€ŒÚ©Ù†Ù‡
2. Label Ù‡Ø§ Ø±Ùˆ Ø¢Ù…Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ù‡ (Ø³ÙˆØ¯Ø¢ÙˆØ±/Ø¶Ø±Ø±)
3. Dataset Ø±Ùˆ Ø¨Ø±Ø§ÛŒ training Ø¢Ù…Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ù‡
"""

import pandas as pd
import numpy as np
import json
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split


def extract_features_from_signals(df):
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ feature Ù‡Ø§ÛŒ Ø§ØµÙ„ÛŒ Ø§Ø² Ø¬Ø¯ÙˆÙ„ signals
    
    Args:
        df: DataFrame Ø­Ø§ÙˆÛŒ signals + performance
    
    Returns:
        DataFrame Ø¨Ø§ feature Ù‡Ø§ÛŒ Ø¢Ù…Ø§Ø¯Ù‡
    """
    features = pd.DataFrame()
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # 1ï¸âƒ£ RSI Features
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    if 'rsi_values' in df.columns:
        rsi_data = df['rsi_values'].apply(
            lambda x: json.loads(x) if isinstance(x, str) else x
        )
        
        features['rsi_1m'] = rsi_data.apply(lambda x: x.get('1m', 50) if x else 50)
        features['rsi_5m'] = rsi_data.apply(lambda x: x.get('5m', 50) if x else 50)
        features['rsi_15m'] = rsi_data.apply(lambda x: x.get('15m', 50) if x else 50)
        features['rsi_1h'] = rsi_data.apply(lambda x: x.get('1h', 50) if x else 50)
        features['rsi_4h'] = rsi_data.apply(lambda x: x.get('4h', 50) if x else 50)
        
        # RSI Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†
        features['rsi_avg'] = features[['rsi_1m', 'rsi_5m', 'rsi_15m']].mean(axis=1)
        
        # RSI Ø§Ù†Ø­Ø±Ø§Ù Ù…Ø¹ÛŒØ§Ø± (convergence)
        features['rsi_std'] = features[['rsi_1m', 'rsi_5m', 'rsi_15m']].std(axis=1)
        
        # Ø¢ÛŒØ§ oversold/overbought Ù‡Ø³ØªØŸ
        features['is_oversold'] = (features['rsi_avg'] < 30).astype(int)
        features['is_overbought'] = (features['rsi_avg'] > 70).astype(int)
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # 2ï¸âƒ£ Score & Confidence Features
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    if 'score' in df.columns:
        features['score'] = df['score'].fillna(0)
        features['score_abs'] = np.abs(features['score'])
        features['score_direction'] = np.sign(features['score'])
    
    if 'advance_score' in df.columns:
        features['advance_score'] = df['advance_score'].fillna(0)
    
    if 'confidence' in df.columns:
        features['confidence'] = df['confidence'].fillna(50)
    
    if 'quality' in df.columns:
        features['quality'] = df['quality'].fillna(50)
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # 3ï¸âƒ£ Trend Features
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    if 'signal_type' in df.columns:
        # Parse JSON
        signal_type_data = df['signal_type'].apply(
            lambda x: json.loads(x) if isinstance(x, str) else {}
        )
        
        # Count trends
        features['trend_up_count'] = signal_type_data.apply(
            lambda x: list(x.values()).count('up') if isinstance(x, dict) else 0
        )
        features['trend_down_count'] = signal_type_data.apply(
            lambda x: list(x.values()).count('down') if isinstance(x, dict) else 0
        )
        features['trend_convergence'] = (features['trend_up_count'] + 
                                         features['trend_down_count'])
    
    if 'convergence_count' in df.columns:
        features['convergence_count'] = df['convergence_count'].fillna(0)
    
    if 'price_trend' in df.columns:
        # Encode categorical
        trend_map = {'up': 1, 'down': -1, 'neutral': 0}
        features['price_trend_encoded'] = df['price_trend'].map(trend_map).fillna(0)
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # 4ï¸âƒ£ Method Feature (Ú©Ø¯ÙˆÙ… Ø±ÙˆØ´ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡)
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    if 'testmode' in df.columns:
        # One-hot encoding
        method_dummies = pd.get_dummies(df['testmode'], prefix='method')
        features = pd.concat([features, method_dummies], axis=1)
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # 5ï¸âƒ£ Time Features
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    if 'entry_time' in df.columns:
        df['entry_time'] = pd.to_datetime(df['entry_time'])
        features['hour'] = df['entry_time'].dt.hour
        features['day_of_week'] = df['entry_time'].dt.dayofweek
        features['is_weekend'] = (features['day_of_week'] >= 5).astype(int)
    
    return features


def create_ml_dataset(cursor, target_period='1h', min_confidence=0):
    """
    Ø³Ø§Ø®Øª dataset Ú©Ø§Ù…Ù„ Ø¨Ø±Ø§ÛŒ ML
    
    Args:
        cursor: database cursor
        target_period: '15m', '30m', '1h', '4h', '24h'
        min_confidence: Ø­Ø¯Ø§Ù‚Ù„ confidence Ø¨Ø±Ø§ÛŒ ÙÛŒÙ„ØªØ±
    
    Returns:
        X, y, feature_names
    """
    # Ú¯Ø±ÙØªÙ† Ø¯ÛŒØªØ§
    query = """
        SELECT 
            sp.*,
            s.rsi_values,
            s.signal_type,
            s.signal_label,
            s.convergence_count,
            s.price_trend,
            s.advance_score,
            s.score,
            s.quality,
            s.testmode
        FROM signal_performance sp
        JOIN signals s ON sp.signal_id = s.id
        WHERE sp.change_1h IS NOT NULL
    """
    
    if min_confidence > 0:
        query += f" AND sp.confidence >= {min_confidence}"
    
    df = pd.read_sql_query(query, cursor.connection)
    
    if df.empty:
        print("âš ï¸ No data available!")
        return None, None, None
    
    print(f"ğŸ“Š Total records: {len(df)}")
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ features
    features = extract_features_from_signals(df)
    
    # Target variable
    target_col = f'is_profitable_{target_period}'
    if target_col not in df.columns:
        print(f"âš ï¸ Target column {target_col} not found!")
        return None, None, None
    
    y = df[target_col].fillna(0).astype(int)
    
    # Ø­Ø°Ù Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§ÛŒ Ø¨Ø§ target null
    valid_mask = df[target_col].notna()
    features = features[valid_mask]
    y = y[valid_mask]
    
    # Ø­Ø°Ù NaN Ù‡Ø§
    features = features.fillna(0)
    
    print(f"âœ… Features: {features.shape[1]}")
    print(f"âœ… Samples: {len(features)}")
    print(f"âœ… Positive samples: {y.sum()} ({y.mean()*100:.1f}%)")
    
    return features, y, features.columns.tolist()


def prepare_train_test_split(X, y, test_size=0.2, random_state=42):
    """
    ØªÙ‚Ø³ÛŒÙ… Ø¯ÛŒØªØ§ Ø¨Ù‡ train/test
    """
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state, stratify=y
    )
    
    print(f"\nğŸ“Š Train/Test Split:")
    print(f"   Train: {len(X_train)} samples")
    print(f"   Test:  {len(X_test)} samples")
    print(f"   Train positive: {y_train.sum()} ({y_train.mean()*100:.1f}%)")
    print(f"   Test positive:  {y_test.sum()} ({y_test.mean()*100:.1f}%)")
    
    return X_train, X_test, y_train, y_test


def normalize_features(X_train, X_test):
    """
    Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ features Ø¨Ø§ StandardScaler
    """
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    return X_train_scaled, X_test_scaled, scaler


def analyze_feature_importance(features, y, feature_names):
    """
    ØªØ­Ù„ÛŒÙ„ Ø§Ù‡Ù…ÛŒØª features Ø¨Ø§ correlation
    """
    from scipy.stats import pointbiserialr
    
    print(f"\n{'â•'*80}")
    print(f"ğŸ“Š FEATURE IMPORTANCE ANALYSIS")
    print(f"{'â•'*80}")
    
    correlations = []
    
    for i, feature in enumerate(feature_names):
        try:
            corr, pval = pointbiserialr(y, features.iloc[:, i])
            correlations.append({
                'feature': feature,
                'correlation': abs(corr),
                'direction': 'positive' if corr > 0 else 'negative',
                'p_value': pval
            })
        except:
            pass
    
    # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ correlation
    correlations = sorted(correlations, key=lambda x: x['correlation'], reverse=True)
    
    print(f"\nğŸ” Top 15 Most Important Features:\n")
    print(f"{'Feature':<30} {'Correlation':<12} {'Direction':<12} {'P-value':<10}")
    print("-" * 80)
    
    for i, item in enumerate(correlations[:15], 1):
        print(f"{item['feature']:<30} {item['correlation']:<12.4f} "
              f"{item['direction']:<12} {item['p_value']:<10.4f}")
    
    return correlations


def export_processed_dataset(X, y, feature_names, filename='ml_ready_dataset.csv'):
    """
    Export dataset Ø¢Ù…Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ ML
    """
    df_export = pd.DataFrame(X, columns=feature_names)
    df_export['target'] = y
    df_export.to_csv(filename, index=False)
    
    print(f"\nâœ… Dataset exported: {filename}")
    print(f"   Shape: {df_export.shape}")
    
    return filename


if __name__ == "__main__":
    import sqlite3
    
    # Test
    conn = sqlite3.connect("data.db")
    cursor = conn.cursor()
    
    print("ğŸš€ Creating ML Dataset...\n")
    
    X, y, feature_names = create_ml_dataset(cursor, target_period='1h', min_confidence=50)
    
    if X is not None:
        # Feature importance
        analyze_feature_importance(X, y, feature_names)
        
        # Train/test split
        X_train, X_test, y_train, y_test = prepare_train_test_split(X, y)
        
        # Export
        export_processed_dataset(X, y, feature_names)
    
    conn.close()